{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "real_data = pd.read_csv('real_ctdata.csv')\n",
    "syn_data = pd.read_csv('ctgan_synthetic_382.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.drop([\"subject_id\"], axis = 1, inplace = True)\n",
    "syn_data.drop([\"subject_id\"], axis = 1, inplace = True)\n",
    "real_data = real_data.loc[:, ~real_data.columns.str.contains('^Unnamed')]\n",
    "syn_data = syn_data.loc[:, ~syn_data.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = real_data\n",
    "x_test = syn_data\n",
    "cat_mask = x_train.dtypes == object\n",
    "cat_list = x_train.columns[cat_mask].tolist()\n",
    "# Or mask categorical columns manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = x_train.select_dtypes(include=[object])\n",
    "cat_test = x_test.select_dtypes(include=[object])\n",
    "num_train = x_train.select_dtypes(include=[float])\n",
    "num_test =  x_test.select_dtypes(include=[float])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Encode Categorical Columns\n",
    "\n",
    "We transform the categorical columns into numerical value using label encoder on top of which we apply one-hot encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 15)\n",
      "(382, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# For training set\n",
    "le = LabelEncoder()\n",
    "cat_train_2 = cat_train.apply(le.fit_transform)\n",
    "oe = OneHotEncoder(sparse=False)\n",
    "cat_train_oe = oe.fit_transform(cat_train_2)\n",
    "cat_train_oe = pd.DataFrame(cat_train_oe)\n",
    "\n",
    "# For test set\n",
    "cat_test_2 = cat_test.apply(le.fit_transform)\n",
    "cat_test_oe = oe.fit_transform(cat_test_2)\n",
    "cat_test_oe = pd.DataFrame(cat_test_oe)\n",
    "\n",
    "# print out the shapes\n",
    "print(cat_train_oe.shape)\n",
    "print(cat_test_oe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 1)\n",
      "(382, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# For training set\n",
    "num_train = scaler.fit_transform(num_train)\n",
    "num_train = pd.DataFrame(num_train)\n",
    "# For testing set\n",
    "num_test = scaler.fit_transform(num_test)\n",
    "num_test = pd.DataFrame(num_test)\n",
    "# print out the shapes\n",
    "print(num_train.shape)\n",
    "print(num_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 16)\n",
      "(382, 16)\n"
     ]
    }
   ],
   "source": [
    "# Integrate datasets\n",
    "x_train = pd.concat([cat_train_oe, num_train], axis=1, sort=False)\n",
    "x_test = pd.concat([cat_test_oe, num_test], axis=1, sort=False)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "# Flatten the data into vectors\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Structure Building\n",
    "\n",
    "A single fully-connected neural layer as encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "latent_dim = 1  # 1 representation vector\n",
    "original_dim= x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "input_data = Input(shape=(original_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(latent_dim, activation='relu')(input_data)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(original_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction (Define a model that would turn input_data into decoded output)\n",
    "autoencoder = Model(input_data, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate encoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_data, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the decoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (assigned # of dimensions) input\n",
    "encoded_input = Input(shape=(latent_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder for 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_testdata = encoder.predict(x_test)\n",
    "decoded_testdata = decoder.predict(encoded_testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_traindata = encoder.predict(x_train)\n",
    "decoded_traindata = decoder.predict(encoded_traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decide the bins by yourself:\n",
    "# The upper bound should be 2 more steps more than the maximum value of both vectors\n",
    "# Controling the whole length of the bins to around 200 woyld be optimal \n",
    "\n",
    "bins = np.arange(0,2.3,0.01)\n",
    "\n",
    "real_inds = pd.DataFrame(np.digitize(encoded_traindata, bins), columns = ['inds'])\n",
    "syn_inds = pd.DataFrame(np.digitize(encoded_testdata, bins), columns = ['inds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probs(table,column):\n",
    "    counts = table[column].value_counts()\n",
    "    freqs = {counts.index[i]: counts.values[i] for i in range(len(counts.index))}\n",
    "    for i in range(1, len(bins)+1):\n",
    "        if i not in freqs.keys():\n",
    "            freqs[i] = 0\n",
    "    sorted_freqs = {}\n",
    "    for k in sorted(freqs.keys()):\n",
    "        sorted_freqs[k] = freqs[k]\n",
    "    probs = []\n",
    "    for k,v in sorted_freqs.items():\n",
    "        probs.append(v/len(table[column]))\n",
    "    return sorted_freqs, np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "real_p = identify_probs(real_inds,'inds')[1]\n",
    "syn_p = identify_probs(syn_inds,'inds')[1]\n",
    "def cos_similarity(p,q):\n",
    "    return 1 - distance.cosine(p, q)\n",
    "cos_similarity(real_p,syn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# T-sne visualization\n",
    "tsne = TSNE(n_components = 2, random_state = 0)\n",
    "tsne_train = tsne.fit_transform(encoded_traindata)\n",
    "tsne_test = tsne.fit_transform(encoded_testdata)\n",
    "tsne_train_df = pd.DataFrame(data = tsne_train, columns = ('Dim_1','Dim_2'))\n",
    "tsne_test_df = pd.DataFrame(data = tsne_test, columns = ('Dim_1','Dim_2'))\n",
    "\n",
    "plt.figure(figsize = [14, 5])\n",
    "plt.subplot(121)\n",
    "plt.title('Original dataset')\n",
    "plt.scatter(tsne_train_df['Dim_1'],tsne_train_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "plt.axis([-30, 40, -40, 40])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Synthetic dataset')\n",
    "plt.scatter(tsne_test_df['Dim_1'],tsne_test_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "plt.axis([-30, 40, -40, 40])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# T-sne visualization\n",
    "pca = PCA(n_components=2, random_state = 0)\n",
    "pca_train = pca.fit_transform(encoded_traindata)\n",
    "pca_test = pca.fit_transform(encoded_testdata)\n",
    "pca_train_df = pd.DataFrame(data = pca_train, columns = ('Dim_1','Dim_2'))\n",
    "pca_test_df = pd.DataFrame(data = pca_test, columns = ('Dim_1','Dim_2'))\n",
    "\n",
    "plt.figure(figsize = [14, 5])\n",
    "plt.subplot(121)\n",
    "plt.title('Original dataset')\n",
    "plt.scatter(pca_train_df['Dim_1'],pca_train_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "plt.axis([-1.0, 2.0, -0.5, 1.5])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Synthetic dataset')\n",
    "plt.scatter(pca_test_df['Dim_1'],pca_test_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "plt.axis([-1.0, 2.0, -0.5, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
